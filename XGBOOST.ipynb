{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "from gensim.models import Phrases\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n",
      "Hi\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1370626</th>\n",
       "      <td>0</td>\n",
       "      <td>lmao that funni but true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82069</th>\n",
       "      <td>1</td>\n",
       "      <td>i miss u and twitter so much my bfs phone is w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489496</th>\n",
       "      <td>0</td>\n",
       "      <td>dalilah got me a francisco rodriguez angel bas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424058</th>\n",
       "      <td>0</td>\n",
       "      <td>had a good night at rutherglen half my famili ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868896</th>\n",
       "      <td>0</td>\n",
       "      <td>meanwhil on other news ryan gigg just score</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                               text\n",
       "1370626       0                           lmao that funni but true\n",
       "82069         1  i miss u and twitter so much my bfs phone is w...\n",
       "1489496       0  dalilah got me a francisco rodriguez angel bas...\n",
       "1424058       0  had a good night at rutherglen half my famili ...\n",
       "868896        0        meanwhil on other news ryan gigg just score"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocess_text as pt\n",
    "\n",
    "#import our function to import data and clean it \n",
    "train_df,val_df = pt.suicidal_intent_data_load(test_dataset = False,remove_stopwords = False,standardization=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.concat([train_df,val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# create a CountVectorizer object\n",
    "count_vec = CountVectorizer()\n",
    "\n",
    "# fit \n",
    "count_vec.fit(new_df.text)\n",
    "#transform\n",
    "x_train_cvec = count_vec.transform(train_df.text)\n",
    "#x_test_cvec  = count_vec.fit_transform(test_df.text)\n",
    "val_df_cvec  = count_vec.transform(val_df.text)\n",
    "\n",
    "#new_df_cvec = count_vec.fit_transform(new_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer(max_features=3891472, ngram_range=(1, 2))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=3891472, ngram_range=(1, 2))</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "TfidfVectorizer(max_features=3891472, ngram_range=(1, 2))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a TF-IDF Vectorizer object\n",
    "tfidf_vec = TfidfVectorizer(ngram_range=(1,2),max_features=3891472)\n",
    "#fit\n",
    "#tfidf_vec.fit(train_df.text)\n",
    "tfidf_vec.fit(new_df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform\n",
    "x_train_tfidf = tfidf_vec.transform(train_df.text)\n",
    "#x_test_tfidf  = tfidf_vec.transform(test_df.text)\n",
    "val_df_tfidf  = tfidf_vec.transform(val_df.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Tokenize the sentences in the train DataFrame\n",
    "sentences = train_df['text'].apply(lambda x: x.split()).tolist()\n",
    "# Train your Word2Vec model\n",
    "model = gensim.models.Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define generate embeddings function\n",
    "def generate_embeddings(text, model):\n",
    "    words = text.split()\n",
    "    embeddings = []\n",
    "    for word in words:\n",
    "        if word in  model.wv.key_to_index:\n",
    "            embeddings.append(model.wv.get_vector(word, norm=True))\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return np.zeros(model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['embeddings'] = train_df['text'].apply(lambda x: generate_embeddings(x, model))\n",
    "val_df['embeddings'] = val_df['text'].apply(lambda x: generate_embeddings(x, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_w2vec = np.stack(train_df['embeddings'].values)\n",
    "y_train_w2vec = train_df['target'].values\n",
    "\n",
    "val_w2vec = np.stack(val_df['embeddings'].values)\n",
    "y_val_w2vec = val_df['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> XGBoost </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (precision_score, \n",
    "                             recall_score, \n",
    "                             f1_score, \n",
    "                             classification_report,\n",
    "                             accuracy_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with TF IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression on TF-IDF\n",
      "Accuracy: 0.78\n",
      "Precision: 0.81\n",
      "Recall: 0.75\n",
      "F1 score: 0.77\n"
     ]
    }
   ],
   "source": [
    "XGBReg = XGBClassifier(max_depth=10,n_jobs=6)\n",
    "XGBReg.fit(x_train_tfidf, train_df.target)\n",
    "y_pred = XGBReg.predict(val_df_tfidf)\n",
    "accuracy = accuracy_score(val_df.target, y_pred)\n",
    "precision = precision_score(val_df.target, y_pred)\n",
    "recall = recall_score(val_df.target, y_pred)\n",
    "f1 = f1_score(val_df.target, y_pred)\n",
    "\n",
    "print(f\"Model: Logistic Regression on TF-IDF\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression on CountVectorizer\n",
      "Accuracy: 0.78\n",
      "Precision: 0.80\n",
      "Recall: 0.74\n",
      "F1 score: 0.77\n"
     ]
    }
   ],
   "source": [
    "XGBReg2 = XGBClassifier(max_depth=10,n_jobs=6)\n",
    "XGBReg2.fit(x_train_cvec, train_df.target)\n",
    "y_pred = XGBReg2.predict(val_df_cvec)\n",
    "\n",
    "accuracy = accuracy_score(val_df.target, y_pred)\n",
    "precision = precision_score(val_df.target, y_pred)\n",
    "recall = recall_score(val_df.target, y_pred)\n",
    "f1 = f1_score(val_df.target, y_pred)\n",
    "\n",
    "print(f\"Model: Logistic Regression on CountVectorizer\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost with Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: Logistic Regression on Word2Vec\n",
      "Accuracy: 0.77\n",
      "Precision: 0.77\n",
      "Recall: 0.78\n",
      "F1 score: 0.77\n"
     ]
    }
   ],
   "source": [
    "XGBReg3 = XGBClassifier(max_depth=10,n_jobs=6)\n",
    "XGBReg3.fit(X_train_w2vec,y_train_w2vec )\n",
    "y_pred = XGBReg3.predict(val_w2vec)\n",
    "\n",
    "accuracy = accuracy_score(y_val_w2vec, y_pred)\n",
    "precision = precision_score(y_val_w2vec, y_pred)\n",
    "recall = recall_score(y_val_w2vec, y_pred)\n",
    "f1 = f1_score(y_val_w2vec, y_pred)\n",
    "\n",
    "print(f\"Model: Logistic Regression on Word2Vec\")\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 score: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (tf-gpu)",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
